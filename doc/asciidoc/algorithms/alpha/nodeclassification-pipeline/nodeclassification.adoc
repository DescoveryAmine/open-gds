[[algorithms-ml-nodeclassification-pipelines]]
= Node classification pipelines
:entity: node
:result: predicted property


[abstract]
--
This section describes Node classification pipelines in the Neo4j Graph Data Science library.
--


[[algorithms-ml-nodeclassification-pipelines-intro]]
== Introduction

Node Classification is a common machine learning task applied to graphs: training models to classify nodes.
Concretely, Node Classification models are used to predict the classes of unlabeled nodes as a node properties based on other node properties.
During training, the property representing the class of the node is referred to as the target property.
GDS supports both binary and multi-class node classification.

In GDS, we have Node Classification pipelines which offer an end-to-end workflow, from feature extraction to node classification.
The training pipelines reside in the <<pipeline-catalog-ops,pipeline catalog>>.
When a training pipeline is <<algorithms-ml-nodeclassification-pipelines-train,executed>>, a classification model is created and stored in the <<model-catalog-ops,model catalog>>.

A training pipeline is a sequence of two phases:
[upperroman]
. The graph is augmented with new node properties in a series of steps.
. The augmented graph is used for training a node classification model.

One can <<algorithms-ml-nodeclassification-adding-node-properties,configure>> which steps should be included above.
The steps execute GDS algorithms that create new node properties.
After configuring the node property steps, one can <<algorithms-ml-nodeclassification-adding-features,select>> a subset of node properties to be used as features.
The training phase (II) trains multiple model candidates using cross-validation, selects the best one, and reports relevant performance metrics.

After <<algorithms-ml-nodeclassification-pipelines-train, training the pipeline>>, a classification model is created.
This model includes the node property steps and feature configuration from the training pipeline and uses them to generate the relevant features for classifying unlabeled nodes.
The classification model can be applied to predict the class of previously unseen nodes.
In addition to the predicted class for each node, the predicted probability for each class may also be retained on the nodes.
The order of the probabilities matches the order of the classes registered in the model.

NOTE: <<algorithms-node-classification-pipelines-predict, Classification>> can only be done with a classification model (not with a training pipeline).

The rest of this page is divided as follows:

* <<algorithms-ml-nodeclassification-creating-a-pipeline, Creating a pipeline>>
* <<algorithms-ml-nodeclassification-adding-node-properties, Adding node properties>>
* <<algorithms-ml-nodeclassification-adding-features, Adding features>>
* <<algorithms-ml-nodeclassification-configure-splits, Configuring the node splits>>
* <<algorithms-ml-nodeclassification-configure-model-parameters, Configuring the model parameters>>
* <<algorithms-ml-nodeclassification-pipelines-train, Training the pipeline>>
* <<algorithms-node-classification-pipelines-predict, Applying a classification model to make predictions>>

[[algorithms-ml-nodeclassification-creating-a-pipeline]]
== Creating a pipeline

The first step of building a new pipeline is to create one using `gds.beta.pipeline.nodeClassification.create`.
This stores a trainable pipeline object in the pipeline catalog of type `Node classification training pipeline`.
This represents a configurable pipeline that can later be invoked for training, which in turn creates a classification model.
The latter is also a model which is stored in the catalog with type `NodeClassification`.

=== Syntax

[.pipeline-create-syntax]
--
.Create pipeline syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.create(
  pipelineName: String
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type    | Description
| pipelineName    | String  | The name of the created pipeline.
|===

include::pipelineInfoResult.adoc[]
--

=== Example

[role=query-example,group=nc]
--
.The following will create a pipeline:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.create('pipe')
----

.Results
[opts="header",cols="1,1,1,1,1"]
|===
| name     | nodePropertySteps | featureProperties | splitConfig | parameterSpace
| "pipe"   | []                | []
           | {testFraction=0.3, validationFolds=3}
           | [{useBiasFeature=true, maxEpochs=100, minEpochs=1, penalty=0.0, patience=1, batchSize=100, tolerance=0.001}]
|===
--

This shows that the newly created pipeline does not contain any steps yet, and has defaults for the split and train parameters.

[[algorithms-ml-nodeclassification-adding-node-properties]]
== Adding node properties

A node classification pipeline can execute one or several GDS algorithms in mutate mode that create node properties in the in-memory graph.
Such steps producing node properties can be chained one after another and created properties can later be used as <<algorithms-ml-nodeclassification-adding-features, features>>.
Moreover, the node property steps that are added to the training pipeline will be executed both when <<algorithms-ml-nodeclassification-pipelines-train, training>> a model and when the classification pipeline is <<algorithms-link-prediction-pipelines-predict, applied for classification>>.

The name of the procedure that should be added can be a fully qualified GDS procedure name ending with `.mutate`.
The ending `.mutate` may be omitted and one may also use shorthand forms such as `node2vec` instead of `gds.beta.node2vec.mutate`.

For example, <<algorithms-ml-models-preprocessing, pre-processing algorithms>> can be used as node property steps.

=== Syntax

[.pipeline-add-node-property-syntax]
--
.Add node property syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.addNodeProperty(
  pipelineName: String,
  procedureName: String,
  procedureConfiguration: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name                      | Type    | Description
| pipelineName              | String  | The name of the pipeline.
| procedureName             | String  | The name of the procedure to be added to the pipeline.
| procedureConfiguration    | Map     | The configuration of the procedure, excluding `graphName`, `nodeLabels` and `relationshipTypes`.
|===

include::pipelineInfoResult.adoc[]
--
=== Example

[role=query-example,group=nc]
--
.The following will add a node property step to the pipeline. Here we assume that the input graph contains a property `sizePerStory`.
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.addNodeProperty('pipe', 'alpha.scaleProperties', {
  nodeProperties: 'sizePerStory',
  scaler: 'L1Norm',
  mutateProperty:'scaledSizes'
})
YIELD name, nodePropertySteps
----

.Results
[opts="header",cols="1,9"]
|===
| name     | nodePropertySteps
| "pipe"   | [{name=gds.alpha.scaleProperties.mutate, config={scaler=L1Norm, mutateProperty=scaledSizes, nodeProperties=sizePerStory}}]
|===

The `scaledSizes` property can be later used as a feature.
--


[[algorithms-ml-nodeclassification-adding-features]]
== Adding features

A Node Classification Pipeline allows you to select a subset of the available node properties to be used as features for the machine learning model.
When executing the pipeline, the selected `nodeProperties` must be either present in the input graph, or created by a previous node property step.
For example, the `embedding` property could be created by the previous example, and we expect `numberOfPosts` to already be present in the in-memory graph used as input, at train and predict time.

=== Syntax

[.pipeline-add-feature-syntax]
--
.Adding a feature to a pipeline syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.selectFeatures(
  pipelineName: String,
  nodeProperties: List or String
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name                   | Type            | Description
| pipelineName           | String          | The name of the pipeline.
| nodeProperties         | List or String  | Configuration for splitting the relationships.
|===

include::pipelineInfoResult.adoc[]
--

=== Example

[role=query-example,group=nc]
--
.The following will select features for the pipeline. Here we assume that the input graph contains a property `sizePerStory` and `scaledSizes` was created in a `nodePropertyStep`.
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.selectFeatures('pipe', ['scaledSizes', 'sizePerStory'])
YIELD name, featureProperties
----

.Results
[opts="header",cols="1,1"]
|===
| name     | featureProperties
| "pipe"   | [scaledSizes, sizePerStory]
|===
--

[[algorithms-ml-nodeclassification-configure-splits]]
== Configuring the node splits

Node Classification Pipelines manage splitting the nodes into several sets for training, testing and validating the models defined in the <<algorithms-ml-nodeclassification-configure-model-parameters,parameter space>>.
Configuring the splitting is optional, and if omitted, splitting will be done using default settings.
The splitting configuration of a pipeline can be inspected by using `gds.beta.model.list` and possibly only yielding `splitConfig`.

The node splits are used in the training process as follows:

. The input graph is split into two parts: the train graph and the test graph. See the <<algorithms-ml-nodeclassification-configure-splits-train-test-image,example below>>.
. The train graph is further divided into a number of validation folds, each consisting of a train part and a validation part. See the <<algorithms-ml-nodeclassification-configure-splits-validation-image, animation below>>.
. Each model candidate is trained on each train part and evaluated on the respective validation part.
. The model with the highest average score according to the primary metric will win the training.
. The winning model will then be retrained on the entire train graph.
. The winning model is evaluated on the train graph as well as the test graph.
. The winning model is retrained on the entire original graph.

Below we illustrate an example for a graph with 12 nodes.
First we use a `holdoutFraction` of 0.25 to split into train and test subgraphs.

[[algorithms-ml-nodeclassification-configure-splits-train-test-image]]
image::train-test-splitting/train-test-split.svg[train-test-image,width="500"]

Then we carry out three validation folds, where we first split the train subgraph into 3 disjoint subsets (s1, s2 and s3), and then alternate which subset is used for validation. For each fold, all candidate models are trained in the red nodes, and validated in the green nodes.

[[algorithms-ml-nodeclassification-configure-splits-validation-image]]
image::train-test-splitting/validation-folds-node-classification.gif[validation-folds-image,width="500"]
// The images were generated using arrows.app. The arrow files are stored in the shared google drive
// in "GDS Team (GDS, Morpheus)/Doc Images/train-test-splitting-illustrations-for-docs"
// The GIF was created in https://ezgif.com/maker/ezgif-3-23bccde0-gif with 150 cs between images and crossfade on

=== Syntax

[.pipeline-configure-split-syntax]
--
.Configure the relationship split syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.configureSplit(
  pipelineName: String,
  configuration: Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of Strings,
  splitConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type    | Description
| pipelineName    | String  | The name of the pipeline.
| configuration   | Map     | Configuration for splitting the relationships.
|===

.Configuration
[opts="header",cols="1,1,1,4"]
|===
| Name               | Type    | Default | Description
| validationFolds    | Integer | 3       | Number of divisions of the training graph used during <<algorithms-ml-nodeclassification-pipelines-train,model selection>>.
| testFraction       | Double  | 0.3     | Fraction of the graph reserved for testing. Must be in the range (0, 1). The fraction used for the training is `1 - testFraction`.
|===

include::pipelineInfoResult.adoc[]
--

=== Example

[role=query-example,group=nc]
--
.The following will configure the splitting of the pipeline:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.configureSplit('pipe', {
 testFraction: 0.2,
  validationFolds: 5
})
YIELD splitConfig
----

.Results
[opts="header",cols="1"]
|===
| splitConfig
| {testFraction=0.2, validationFolds=5}
|===

We now reconfigured the splitting of the pipeline, which will be applied during <<algorithms-ml-nodeclassification-pipelines-train, training>>.
--

[[algorithms-ml-nodeclassification-configure-model-parameters]]
== Configuring the model parameters

The `gds.beta.pipeline.nodeClassification.configureParams` mode is used to set up the train mode with a list of configurations of logistic regression models.
The set of model configurations is called the _parameter space_ which parametrizes a set of model candidates.
The parameter space can be configured by passing this procedure a list of maps, where each map configures the training of one logistic regression model.
In <<algorithms-ml-nodeclassification-pipelines-train, Training the pipeline>>, we explain further how the configured model candidates are trained, evaluated and compared.

The allowed model parameters are listed in the table <<nodeclassification-pipelines-model-configuration-table>>.

If `configureParams` is not used, then a single model with defaults for all the model parameters is used.
The parameter space of a pipeline can be inspected using `gds.beta.model.list` and optionally yielding only `parameterSpace`.

=== Syntax

[.pipeline-configure-params-syntax]
--
.Configure the train parameters syntax
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.configureParams(
  pipelineName: String,
  parameterSpace: List of Map
)
YIELD
  name: String,
  nodePropertySteps: List of Map,
  featureProperties: List of String,
  splitConfig: Map,
  parameterSpace: List of Map
----

.Parameters
[opts="header",cols="1,1,4"]
|===
| Name            | Type        | Description
| pipelineName    | String      | The name of the pipeline.
| parameterSpace  | List of Map | The parameter space used to select the best model from. Each Map corresponds to potential model. The allowed parameters for a model are defined in the next table.
|===

[[nodeclassification-pipelines-model-configuration-table]]
.Model configuration
[opts="header",cols="1,1,1m,1,4"]
|===
| Name                | Type    | Default         | Optional | Description
| penalty             | Float   | 0.0             | yes      | Penalty used for the logistic regression. By default, no penalty is applied.
| batchSize           | Integer | 100             | yes      | Number of nodes per batch.
| minEpochs           | Integer | 1               | yes      | Minimum number of training epochs.
| maxEpochs           | Integer | 100             | yes      | Maximum number of training epochs.
| patience            | Integer | 1               | yes      | Maximum number of unproductive consecutive epochs.
| tolerance           | Float   | 0.001           | yes      | The minimal improvement of the loss to be considered productive.
|===

include::pipelineInfoResult.adoc[]
--

=== Example

[role=query-example,group=nc]
--
.The following will configure the parameter space of the pipeline:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.nodeClassification.configureParams('pipe',
  [{penalty: 0.0625}, {tolerance: 0.01}, {maxEpochs: 500}]
) YIELD parameterSpace
----

.Results
[opts="header",cols="1"]
|===
| parameterSpace
| [{useBiasFeature=true, maxEpochs=100, minEpochs=1, penalty=0.0625, patience=1, batchSize=100, tolerance=0.001}, {useBiasFeature=true, maxEpochs=100, minEpochs=1, penalty=0.0, patience=1, batchSize=100, tolerance=0.01}, {useBiasFeature=true, maxEpochs=500, minEpochs=1, penalty=0.0, patience=1, batchSize=100, tolerance=0.001}]
|===

The `parameterSpace` in the pipeline now contains the three different model parameters, expanded with the default values.
Each specified model configuration will be tried out during the model selection in <<algorithms-ml-nodeclassification-pipelines-train, training>>.
--

include::training.adoc[]

include::predict.adoc[]
