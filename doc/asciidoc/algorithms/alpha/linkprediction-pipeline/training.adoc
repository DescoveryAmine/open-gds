[[algorithms-ml-linkprediction-pipelines-train]]
== Training the pipeline

The train mode, `gds.beta.pipeline.linkPrediction.train`, is responsible for splitting data, feature extraction, model selection, training and storing a model for future use.
Running this mode results in a prediction model of type `LinkPrediction` being stored in the <<model-catalog-ops,model catalog>> along with metrics collected during training.
The model can be <<algorithms-link-prediction-pipelines-predict,applied>> to a possibly different graph which produces a relationship type of predicted links, each having a predicted probability stored as a property.

More precisely, the procedure will in order:

. Apply `nodeLabels` and `relationshipType` filters to the graph. All subsequent graphs have the same node set.
. Create a relationship split of the graph into `test`, `train` and `feature-input` sets as described in <<algorithms-ml-linkprediction-configure-splits, Configuring the relationship splits>>.
These graphs are internally managed and exist only for the duration of the training.
. Apply the node property steps, added according to <<algorithms-ml-linkprediction-adding-node-properties, Adding node properties>>, on the `feature-input` graph.
. Apply the feature steps, added according to <<algorithms-ml-linkprediction-adding-features, Adding link features>>, to the `train` graph, which yields for each `train` relationship an _instance_, that is, a feature vector and a binary label.
. Split the training instances using stratified k-fold cross-validation.
The number of folds `k` can be configured using `validationFolds` in `gds.beta.pipeline.linkPrediction.configureSplit`.
. Train each model candidate given by the <<algorithms-ml-linkprediction-configure-model-parameters,parameter space>> for each of the folds and evaluate the model on the respective validation set.
The training process uses a logistic regression algorithm, and the evaluation uses the <<algorithms-ml-linkprediction-pipelines-metrics, AUCPR metric>>.
. Declare as winner the model with the highest average metric across the folds.
. Re-train the winning model on the whole training set and evaluate it on both the `train` and `test` sets.
In order to evaluate on the `test` set, the feature pipeline is first applied again as for the `train` set.
. Register the winning model in the <<model-catalog-ops, Model Catalog>>.

NOTE: The above steps describe what the procedure does logically.
The actual steps as well as their ordering in the implementation may differ.

NOTE: A step can only use node properties that are already present in the input graph or produced by steps, which were added before.


=== Syntax

[.include-with-train]
--
.Run Link Prediction in train mode on a named graph:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.train(
  graphName: String,
  configuration: Map
) YIELD
  trainMillis: Integer,
  modelInfo: Map,
  modelSelectionStats: Map,
  configuration: Map
----

include::../../common-configuration/common-parameters-named-graph.adoc[]

.Configuration
[opts="header",cols="1,1,1m,1,4"]
|===
| Name                                                          | Type              | Default | Optional | Description
| modelName                                                     | String            | n/a     | no       | The name of the model to train, must not exist in the Model Catalog.
| pipeline                                                      | String            | n/a     | no       | The name of the pipeline to execute.
| negativeClassWeight                                           | Float             | 1.0     | yes      | Weight of negative examples in model evaluation. Positive examples have weight 1. More details <<algorithms-ml-linkprediction-pipelines-classimbalance, here>>.
| randomSeed                                                    | Integer           | n/a     | yes      | Seed for the random number generator used during training.
| <<common-configuration-node-labels,nodeLabels>>               | List of String    | ['*']   | yes      | Filter the named graph using the given node labels.
| <<common-configuration-relationship-types,relationshipTypes>> | List of String    | ['*']   | yes      | Filter the named graph using the given relationship types.
| <<common-configuration-concurrency,concurrency>>              | Integer           | 4       | yes      | The number of concurrent threads used for running the algorithm.
|===


.Results
[opts="header",cols="1,1,6"]
|===
| Name                    | Type    | Description
| trainMillis             | Integer | Milliseconds used for training.
| modelInfo               | Map     | Information about the training and the winning model.
| modelSelectionStats     | Map     | Statistics about evaluated metrics for all model candidates.
| configuration           | Map     | Configuration used for the train procedure.
|===

The `modelInfo` can also be retrieved at a later time by using the <<catalog-model-list, Model List Procedure>>.
The `modelInfo` return field has the following algorithm-specific subfields:

.Model info fields
[opts="header",cols="1,1,6"]
|===
| Name                    | Type          | Description
| bestParameters          | Map           | The model parameters which performed best on average on validation folds according to the primary metric.
| metrics                 | Map           | Map from metric description to evaluated metrics for the winning model over the subsets of the data, see below.
| trainingPipeline        | Map           | The pipeline used for the training.
|===


The structure of `modelInfo` is:

[listing]
----
{
    bestParameters: Map,        // <1>
    trainingPipeline: Map       // <2>
    metrics: {                  // <3>
        AUCPR: {
            test: Float,        // <4>
            outerTrain: Float,  // <5>
            train: {           // <6>
                avg: Float,
                max: Float,
                min: Float,
            },
            validation: {      // <7>
                avg: Float,
                max: Float,
                min: Float
            }
        }
    }
}
----
<1> The best scoring model candidate configuration.
<2> The pipeline used for the training.
<3> The `metrics` map contains an entry for each metric description (currently only `AUCPR`) and the corresponding results for that metric.
<4> Numeric value for the evaluation of the best model on the test set.
<5> Numeric value for the evaluation of the best model on the outer train set.
<6> The `train` entry summarizes the metric results over the `train` set.
<7> The `validation` entry summarizes the metric results over the `validation` set.
--

=== Example

In this example we will create a small graph and use the training pipeline we have built up thus far.
The graph consists of a handful nodes connected in a particular pattern.
The example graph looks like this:

image::example-graphs/link-prediction.svg[Visualization of the example graph,align="center"]

.The following Cypher statement will create the example graph in the Neo4j database:
[source, cypher, role=noplay setup-query, group=lp]
----
CREATE
  (alice:Person {name: 'Alice', numberOfPosts: 38}),
  (michael:Person {name: 'Michael', numberOfPosts: 67}),
  (karin:Person {name: 'Karin', numberOfPosts: 30}),
  (chris:Person {name: 'Chris', numberOfPosts: 132}),
  (will:Person {name: 'Will', numberOfPosts: 6}),
  (mark:Person {name: 'Mark', numberOfPosts: 32}),
  (greg:Person {name: 'Greg', numberOfPosts: 29}),
  (veselin:Person {name: 'Veselin', numberOfPosts: 3}),

  (alice)-[:KNOWS]->(michael),
  (michael)-[:KNOWS]->(karin),
  (michael)-[:KNOWS]->(chris),
  (michael)-[:KNOWS]->(greg),
  (will)-[:KNOWS]->(michael),
  (will)-[:KNOWS]->(chris),
  (mark)-[:KNOWS]->(michael),
  (mark)-[:KNOWS]->(will),
  (greg)-[:KNOWS]->(chris),
  (veselin)-[:KNOWS]->(chris),
  (karin)-[:KNOWS]->(veselin),
  (chris)-[:KNOWS]->(karin);
----

With the graph in Neo4j we can now project it into the graph catalog.
We do this using a native projection targeting the `Person` nodes and the `KNOWS` relationships.
We will also project the `numberOfPosts` property, so it can be used when creating link features.
For the relationships we must use the `UNDIRECTED` orientation.
This is because the Link Prediction pipelines are defined only for undirected graphs.

.The following statement will project a graph using a native projection and store it in the graph catalog under the name 'myGraph'.
[source, cypher, role=noplay graph-project-query, group=lp]
----
CALL gds.graph.project(
  'myGraph',
  {
    Person: {
      properties: ['numberOfPosts']
    }
  },
  {
    KNOWS: {
      orientation: 'UNDIRECTED'
    }
  }
)
----

WARNING: The Link Prediction model requires the graph to be created using the `UNDIRECTED` orientation for relationships.


[[algorithms-ml-linkprediction-pipeline-examples-train-query]]
[role=query-example,group=lp]
--
.The following will train a model using a pipeline:
[source, cypher, role=noplay]
----
CALL gds.beta.pipeline.linkPrediction.train('myGraph', {
  pipeline: 'pipe',
  modelName: 'lp-pipeline-model',
  randomSeed: 42
}) YIELD modelInfo
RETURN
  modelInfo.bestParameters AS winningModel,
  modelInfo.metrics.AUCPR.train.avg AS avgTrainScore,
  modelInfo.metrics.AUCPR.outerTrain AS outerTrainScore,
  modelInfo.metrics.AUCPR.test AS testScore
----

.Results
[opts="header", cols="6, 2, 2, 2"]
|===
| winningModel                                                                                               | avgTrainScore      | outerTrainScore     | testScore
| {useBiasFeature=true, maxEpochs=100, minEpochs=1, penalty=0.0, patience=1, batchSize=100, tolerance=0.001} | 0.3721560846560847 |  0.3801587301587301 | 0.7638888888888888
|===

We can see the model configuration with `tolerance = 0.001` (and defaults filled for remaining parameters) was selected, and has a score of `0.76` on the test set.
The score computed as the <<algorithms-ml-linkprediction-pipelines-metrics, AUCPR>> metric, which is in the range [0, 1].
A model which gives higher score to all links than non-links will have a score of 1.0, and a model that assigns random scores will on average have a score of 0.5.

--
