[[python-client-pipelines]]
= Machine learning pipelines

The Python client has special support for <<algorithms-ml-linkprediction-pipelines>> and <<algorithms-ml-nodeclassification-pipelines>>.
The GDS pipelines are represented as pipeline objects.

The Python method calls to _create_ the pipelines match their Cypher counterparts exactly.
However, the rest of the pipeline functionality is deferred to methods on the pipeline objects themselves.
Once created, the `TrainingPipeline` can be passed as arguments to methods in the Python client, such as the <<pipeline-catalog-ops, pipeline catalog operations>>.
Additionally, the `TrainingPipeline` has convenience methods allowing for inspection of the pipeline represented without explicitly involving the pipeline catalog.

include::python-client-gds-object.adoc[]


== Node classification

This section outlines how to use the Python client to build, configure and train a <<algorithms-ml-nodeclassification-pipelines, node classification pipeline>>, as well as how to use the model that training produces for predictions.


=== Pipeline

The creation of the node classification pipeline is very similar to <<algorithms-ml-nodeclassification-creating-a-pipeline, how it's done in Cypher>>.
To create a new node classification pipeline one would make the following call:

[source,python]
----
pipe, res = gds.beta.pipeline.nodeClassification.create("my-pipe")
----

where `pipe` is a pipeline object, and `res` is a `dict[str, any]` containing metadata from the underlying procedure call.

To then go on to build, configure and train the pipeline we would call methods directly on the node classification pipeline object.
Below is a description of the methods on such objects:

.Node classification pipeline methods
[opts="header",cols="2m,2m,2m,3"]
|===
| Name                    | Arguments              | Return type            | Description
| addNodeProperty         | procedure_name: str, +
                            config: **kwargs       | dict[str, any]         | <<algorithms-ml-nodeclassification-adding-node-properties, Add an algorithm that produces a node property to the pipeline, with optional algorithm-specific configuration>>.
| selectFeatures          | node_properties: +
                            Union[str, list[str]]  | dict[str, any]         | <<algorithms-ml-nodeclassification-adding-features, Select node properties to be used as features>>.
| configureSplit          | config: **kwargs       | dict[str, any]         | <<algorithms-ml-nodeclassification-configure-splits, Configure the train-test dataset split>>.
| configureParams         | parameter_space: +
                            list[dict[str, any]]   | dict[str, any]         | <<algorithms-ml-nodeclassification-configure-model-parameters, Configure the model parameter space used during the model selection phase>>.
| train                   | G: Graph, +
                            config: **kwargs       | NCPredictionPipeline, +
                                                             dict[str, any] | <<algorithms-ml-nodeclassification-pipelines-train, Train the model on the given input graph using given keyword arguments>>.
| node_property_steps     | -                      | list[dict[str, any]]   | Returns the node property steps of the pipeline.
| feature_properties      | -                      | list[dict[str, any]]   | Returns a list of the selected feature properties for the pipeline.
| split_config            | -                      | dict[str, any]         | Returns the configuration set up for train-test splitting of the dataset.
| parameter_space         | -                      | list[dict[str, any]]   | Returns the model parameter space set up for model selection when training.
| name                    | -                      | str                    | The name of the pipeline as it appears in the pipeline catalog.
| type                    | -                      | str                    | The type of pipeline.
| creation_time           | -                      | neo4j.time.Datetime    | Time when the pipeline was created.
| drop                    | -                      | None                   | Removes the model from the <<pipeline-catalog-ops,GDS Pipeline Catalog>>.
| exists                  | -                      | bool                   | `True` if the model exists in the GDS Pipeline Catalog, `False` otherwise.
|===

There are two main differences when comparing the methods above that map to procedures of the Cypher API:

* As the Python methods are called on the pipeline object, one does not need to provide a name when calling them.
* Configuration parameters in the Cypher calls are represented by named keyword arguments in the Python method calls.

Another difference is that the `train` Python call takes a <<python-client-graph-object, graph object>> instead of a graph name, and returns a `NCModel` <<python-client-model-object, model object>> that we can run predictions with as well as a `dict[str, any]` with the metadata from the training.

Please consult the <<algorithms-ml-nodeclassification-pipelines, node classification Cypher documentation>> for information about what kind of input the methods expect.


[[python-client-pipelines-nodeclassification-pipe-example]]
==== Example

Below is a small example of how one could configure and train a very basic node classification pipeline.
Note that we don't configure splits explicitly, but rather use the default.
We suppose that the graph `G` we train on has a node property "my-class" which will be the target of our classification training.

[source,python]
----
pipe, _ = gds.beta.pipeline.nodeClassification.create("my-pipe")

# Add Degree centrality as a property step producing "rank" node properties
pipe.addNodeProperty("degree", mutateProperty="rank")

# Select our "rank" property as a feature for the model training
pipe.selectFeatures("rank")

# Verify that the features to be used in model training are what we expect
steps = pipe.feature_properties()
assert len(steps) == 1
assert steps[0]["feature"] == "rank"

# Configure the model training do to 2-fold cross-validation over tolerance
pipe.configureParams([{"tolerance": 0.01}, {"tolerance": 0.001}])

# Train the pipeline targeting node property "my-class" as label and "ACCURACY" as only metric
trained_pipe_model, res = pipe.train(G, modelName="my-model", targetProperty="my-class", metrics=["ACCURACY"])
assert res["trainMillis"] >= 0
----

A model referred to as "my-model" in the <<model-catalog-ops, GDS Model Catalog>> is produced.
In the next section we will go over how to use that model to make predictions.


=== Model

As we saw in the previous section, node classification models are created when training a node classification pipeline.
In addition to inheriting the methods common to all <<python-client-model-object, model objects>>, node classification models have the following methods:

.Node classification model methods
[opts="header",cols="5m,5m,6m,11"]
|===
| Name           | Arguments        | Return type          | Description
| predict_mutate | G: Graph, +
                   config: **kwargs | dict[str, any]       | <<algorithms-ml-nodeclassification-pipeline-examples-mutate, Predict classes for nodes of the input graph and mutate graph with predictions>>.
| predict_stream | G: Graph, +
                   config: **kwargs | list[dict[str, any]] | <<algorithms-node-classification-pipelines-predict-examples-stream, Predict classes for nodes of the input graph and stream the results>>.
| predict_write | G: Graph, +
                   config: **kwargs | dict[str, any]       | <<algorithms-ml-nodeclassification-pipeline-examples-write, Predict classes for nodes of the input graph and write results back to the database>>.
| metrics        | -                | dict[str, any]       | Returns values for the metrics specified when training, for this particular model.
|===

One can note that the predict methods are indeed very similar to <<algorithms-node-classification-pipelines-predict, their Cypher counterparts>>.
The three main differences are that:

* They take a <<python-client-graph-object, graph object>> instead of a graph name.
* They have Python keyword arguments representing the keys of the configuration map.
* One does not have to provide a "modelName" since the <<python-client-model-object, model object>> used itself have this information.


==== Example (continued)

We now continue the <<python-client-pipelines-nodeclassification-pipe-example, example above>> using the node classification model `trained_pipe_model` we trained there.
Suppose that we have a new graph `H` that we want to run predictions on.

[source,python]
----
# Make sure our model performed well enough on the test set
metrics = trained_pipe_model.metrics()
assert metrics["ACCURACY"]["test"] >= 0.85

# Predict on `H` and stream the results with a specific concurrency of 8
result = trained_pipe_model.predict_stream(H, concurrency=8)
assert len(results) == H.node_count()
----


== Link prediction

This section outlines how to use the Python client to build, configure and train a <<algorithms-ml-linkprediction-pipelines, link prediction pipeline>>, as well as how to use the model that training produces for predictions.


=== Pipeline

The creation of the link prediction pipeline is very similar to <<algorithms-ml-linkprediction-creating-a-pipeline, how it's done in Cypher>>.
To create a new link prediction pipeline one would make the following call:


[source,python]
----
pipe, res = gds.beta.pipeline.linkPrediction.create("my-pipe")
----

where `pipe` is a pipeline object, and `res` is a `dict[str, any]` containing metadata from the underlying procedure call.

To then go on to build, configure and train the pipeline we would call methods directly on the link prediction pipeline object.
Below is a description of the methods on such objects:

.Link prediction pipeline methods
[opts="header",cols="2m,2m,2m,3"]
|===
| Name                | Arguments              | Return type             | Description
| addNodeProperty     | procedure_name: str, +
                        config: **kwargs       | dict[str, any]          | <<algorithms-ml-linkprediction-adding-node-properties, Add an algorithm that produces a node property to the pipeline, with optional algorithm-specific configuration>>.
| addFeature          | feature_type: str, +
                        config: **kwargs       | dict[str, any]          | <<algorithms-ml-linkprediction-adding-features, Add a link feature for model training based on node properties and a feature combiner>>.
| configureSplit      | config: **kwargs       | dict[str, any]          | <<algorithms-ml-linkprediction-configure-splits, Configure the feature-train-test dataset split>>.
| configureParams     | parameter_space: +
                        list[dict[str, any]]   | dict[str, any]          | <<algorithms-ml-linkprediction-configure-model-parameters, Configure the model parameter space used during the model selection phase>>.
| train               | G: Graph, +
                        config: **kwargs       | LPPredictionPipeline, +
                                                 dict[str, any]          | <<algorithms-ml-linkprediction-pipelines-train, Train the model on the given input graph using given keyword arguments>>.
| node_property_steps | -                      | list[dict[str, any]]    | Returns the node property steps of the pipeline.
| feature_steps       | -                      | list[dict[str, any]]    | Returns a list of the selected feature steps for the pipeline.
| split_config        | -                      | dict[str, any]          | Returns the configuration set up for feature-train-test splitting of the dataset.
| parameter_space     | -                      | list[dict[str, any]]    | Returns the model parameter space set up for model selection when training.
| name                | -                      | str                     | The name of the pipeline as it appears in the pipeline catalog.
| type                | -                      | str                     | The type of pipeline.
| creation_time       | -                      | neo4j.time.Datetime     | Time when the pipeline was created.
| drop                | -                      | None                    | Removes the model from the <<pipeline-catalog-ops,GDS Pipeline Catalog>>.
| exists              | -                      | bool                    | `True` if the model exists in the GDS Pipeline Catalog, `False` otherwise.
|===

There are two main differences when comparing the methods above that map to procedures of the Cypher API:

* As the Python methods are called on the pipeline object, one does not need to provide a name when calling them.
* Configuration parameters in the Cypher calls are represented by named keyword arguments in the Python method calls.

Another difference is that the `train` Python call takes a <<python-client-graph-object, graph object>> instead of a graph name, and returns a `LPModel` <<python-client-model-object, model object>> that we can run predictions with as well as a `dict[str, any]` with the metadata from the training.

Please consult the <<algorithms-ml-linkprediction-pipelines, link prediction Cypher documentation>> for information about what kind of input the methods expect.


[[python-client-pipelines-linkprediction-pipe-example]]
==== Example

Below is a small example of how one could configure and train a very basic link prediction pipeline.
Note that we don't configure training parameters explicitly, but rather use the default.
Suppose we have a graph `G` that we want to train our pipeline on.

[source,python]
----
pipe, _ = gds.beta.pipeline.linkPrediction.create("my-pipe")

# Add FastRP as a property step producing "embedding" node properties
pipe.addNodeProperty("fastRP", embeddingDimension=128, mutateProperty="embedding")

# Combine our "embedding" node properties with Hadamard to create link features for training
pipe.addFeature("hadamard", nodeProperties=["embedding"])

# Verify that the features to be used in model training are what we expect
steps = pipe.feature_steps()
assert len(steps) == 1
assert steps[0]["name"] == "HADAMARD"

# Specify the fractions we want for our dataset split
pipe.configureSplit(trainFraction=0.2, testFraction=0.1)

# Train the pipeline and produce a model named "my-model"
trained_pipe_model, res = pipe.train(G, modelName="my-model")
assert res["trainMillis"] >= 0
----

A model referred to as "my-model" in the <<model-catalog-ops, GDS Model Catalog>> is produced.
In the next section we will go over how to use that model to make predictions.

=== Model

As we saw in the previous section, link prediction models are created when training a link prediction pipeline.
In addition to inheriting the methods common to all <<python-client-model-object, model objects>>, link prediction models have the following methods:

.Link prediction model methods
[opts="header",cols="5m,5m,6m,11"]
|===
| Name           | Arguments        | Return type          | Description
| predict_mutate | G: Graph, +
                   config: **kwargs | dict[str, any]       | <<algorithms-link-prediction-pipelines-predict-examples-mutate, Predict links between non-neighboring nodes of the input graph and mutate graph with predictions>>.
| predict_stream | G: Graph, +
                   config: **kwargs | list[dict[str, any]] | <<algorithms-link-prediction-pipelines-predict-examples-stream, Predict links between non-neighboring nodes of the input graph and stream the results>>.
| metrics        | -                | dict[str, any]       | Returns values for the metrics used when training, for this particular model.
|===

One can note that the predict methods are indeed very similar to <<algorithms-link-prediction-pipelines-predict, their Cypher counterparts>>.
The three main differences are that:

* They take a <<python-client-graph-object, graph object>> instead of a graph name.
* They have Python keyword arguments representing the keys of the configuration map.
* One does not have to provide a "modelName" since the <<python-client-model-object, model object>> used itself have this information.


==== Example (continued)

We now continue the <<python-client-pipelines-linkprediction-pipe-example, example above>> using the link prediction model `trained_pipe_model` we trained there.
Suppose that we have a new graph `H` that we want to run predictions on.

[source,python]
----
# Make sure our model performed well enough on the test set
metrics = trained_pipe_model.metrics()
assert metrics["AUCPR"]["test"] >= 0.85

# Predict on `H` and mutate it with the relationship predictions
results = trained_pipe_model.predict_mutate(H, topN=5, mutateRelationshipType="PRED_REL")
assert result["relationshipsWritten"] == 5 * 2  #  Undirected relationships written
----


== The pipeline catalog

The primary way to use pipeline objects is for training models.
Additionally, pipeline objects can be used as input to <<pipeline-catalog-ops, GDS Pipeline Catalog operations>>.
For instance, supposing we have a pipeline object `pipe`, we could:

[source,python]
----
catalog_contains_pipe = gds.beta.pipeline.exists(pipe)

gds.beta.model.drop(pipe) #  same as pipe.drop()
----

